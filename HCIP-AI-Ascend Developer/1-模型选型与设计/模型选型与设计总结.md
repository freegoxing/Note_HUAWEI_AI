## 网络结构设计的一些基本要素
网络结构是模型的“骨架”，需通过层与模块的组合，让模型能有效捕捉数据规律。设计核心逻辑：“用最小的复杂度实现任务所需的特征提取能力”。
#### 控制网络深度与宽度： 平衡拟合能力与效率
- 深度：深度不足会导致 “欠拟合”，过深会导致 “梯度消失 / 爆炸” 和计算量激增。
- 在设计网络结构时，可以用残差连接（[[基于卷积神经网络的视觉模型#ResNet|ResNet]]）、跳跃连接（[[基于卷积神经网络的视觉模型#DenseNet|DenseNet]]）让梯度直接回传；用归一化层（[[基于卷积神经网络的视觉模型#批量规范化（batch normalization）|BN]]/LN）稳定训练。
- 宽度：指的是每层神经元/通道数，宽度不足会限制特征表达能力，过宽会增加冗余计算。
- 浅层（靠近输入）宽度小（提取基础特征，如边缘），深层宽度大（提取复杂特征，如物体部件）。例如：ResNet-50 的通道数从 64→256→512→1024→2048 逐步增加。
#### 加入正则化与适配层： 提升泛化与兼容性
[[../../人工智能-原理与运用/03神经网络基础/参数优化方法#参数正则化|正则化]]的加入可以减少模型过拟合，例如使用[[../../人工智能-原理与运用/03神经网络基础/参数优化方法#Dropout 正则化|Dropout]]、[[../../人工智能-原理与运用/03神经网络基础/参数优化方法#L1正则化|L1正则]]、[[../../人工智能-原理与运用/03神经网络基础/参数优化方法#L2正则化|L2正则]]等
## 选择合适的激活函数
- 在CNN和MLP的网络中，[[../../人工智能-原理与运用/03神经网络基础/激活函数#ReLU 函数|ReLU]]及其变体的激活函数是首选，因为它提供了非线性能力， 同时计算简单，还能缓解梯度消失或爆炸。 
- 在RNN/Transformer结构中，优先使用平滑激活函数，这样可以避免输出突变。例如 GELU，而在更深层的网络结构中，[[../../人工智能-原理与运用/03神经网络基础/激活函数#Swish 函数|Swish 函数]]更适合。 
- 在输出层，二分类使用[[../../人工智能-原理与运用/03神经网络基础/激活函数#Sigmoid 函数|Sigmoid]]，对分类更适合[[../../人工智能-原理与运用/03神经网络基础/激活函数#Softmax 函数|Softmax]]，而回归任务可以使用[[../../人工智能-原理与运用/03神经网络基础/激活函数#Tanh 函数|Tanh]]或者不适用激活函数。 
- 需要注意：在深层次网络结构中，尽可能避免使用[[../../人工智能-原理与运用/03神经网络基础/激活函数#Sigmoid 函数|Sigmoid]]和[[../../人工智能-原理与运用/03神经网络基础/激活函数#Tanh 函数|Tanh]]。
```ad-note
title:Sigmoid和Tanh的饱和性
观察它们的函数图像，当输入值（x）的绝对值很大时，函数曲线会变得非常平缓，进入“饱和区”。在饱和区，函数的**导数（梯度）接近于0**,-在深层网络中，梯度是逐层相乘的。如果每一层的梯度都小于1，那么经过多个层连乘之后，反向传播到较浅层（靠近输入层）的梯度会变得**指数级地小**，最终趋近于零。- 浅层的权重几乎得不到有效的更新。因为浅层网络通常负责提取基础、通用的特征（如边缘、角落），如果它们无法被训练，整个网络的性能会受到严重限制。网络训练会变得极其缓慢，甚至完全停止
```
## 选择适合的损失函数
在分类任务中，最常见的是使用交叉熵损失函数，
回归任务常用均方误差损失函数。
但是，在更多的场景中，损失函数是根据任务来特定设置的。
