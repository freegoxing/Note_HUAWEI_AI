#LeNet #AlexNet #NiN #GoogLeNet #Inception #BN #ResNet #DenseNet
# 图像分类任务
### 从多层感知机到卷积神经网络
由于由全连接神经网络构成的多层感知机参数量较大，若构建深层次的神经网络，需要消耗更多的计算资源
卷积核的权值共享大幅降低了神经网络的参数量， 同时为探索更多图像识别功能提供了新的工具。
### 从LeNet到AlexNet
LeNet，它是最早发布的卷积神经网络之一，因其在计算机视觉任务中的高效性能而受到广泛关注。
AlexNet 横空出世。它首次证明了学习到的特征可以超越手工设计的特征。它一举打破了计算机视觉研究的现状。 AlexNet使用了8层卷积神经网络
![[HCIP-AI-Ascend Developer V2.0 培训教材 .pdf#page=203&rect=163,464,384,641&color=red|HCIP-AI-Ascend Developer V2.0 培训教材 , p.203|600]]
## VGG
虽然AlexNet证明深层神经网络卓有成效，但它没有提供一个通用的模板来指导后续的研究人员设计新的网络。
VGG块形成了一个可通用网络结构，通过堆叠块的方式，快速增加模型的深度，提升模型识别能力
- 网络中使用了带填充的卷积层 
- 卷积核尺寸为3 X 3 
- 使用了最大池化层
![[HCIP-AI-Ascend Developer V2.0 培训教材 .pdf#page=204&rect=196,451,349,624&color=red|HCIP-AI-Ascend Developer V2.0 培训教材 , p.204|500]]
## NiN（network in network）网络
LeNet、AlexNet和VGG都有一个共同的设计模式：通过一系列的卷积层与池化来提取空间结构特征；然后通过全连接层对特征的表征进行处理。
NiN网络的思想是使用1 * 1的卷积层增加网络深度，提高模型的非线性表达能力 （使用激活函数），并且在最后使用全局池化层代替全连接的最后一层，这样即增加了网络深度，同时大幅降低模型参数量
![[HCIP-AI-Ascend Developer V2.0 培训教材 .pdf#page=205&rect=391,453,480,611&color=red|HCIP-AI-Ascend Developer V2.0 培训教材 , p.205|400]]
## GoogLeNet
![[HCIP-AI-Ascend Developer V2.0 培训教材 .pdf#page=206&rect=161,556,381,623&color=red|HCIP-AI-Ascend Developer V2.0 培训教材 , p.206]]
这个模型一个重点是使用了不同大小的卷积核，解决了如何选择不同大小的卷积核是最合适的问题
它在网络架构中使用了名为Inception 块的架构
Inception块由四条并行路径组成。前三条路径使用窗口大小为1×1、3×3和5×5的卷积层，从不同空间大小中提取信息。中间的两条路径在输入上执行卷积，以减少通道数，从而降低模型的复杂性。第四条路径使用最大池化层，然后使用卷积层来改变通道数
## Inception架构的发展
Inception架构是GoogLeNet最重要的部分，它也经历几次发展，下面讨论一下它们的发展思想
![[HCIP-AI-Ascend Developer V2.0 培训教材 .pdf#page=207&rect=84,452,480,624&color=red|HCIP-AI-Ascend Developer V2.0 培训教材 , p.207]]
## 批量规范化（batch normalization）
随着神经网络的层数增加，模型越来越深。数据在底层传到上层时，很容易落入激活函数的梯度饱和区。另外，同一分布的数据经过多层的变化，每一层的参数更新都会导致上层的输入数据在输出时分布规律发生了变化，并且这个差异会随着网络深度增大而增大，这会导致： 
- 整个模型中的数据分布不同，模型训练较难 
- 部分层输出落在激活函数的梯度饱和区，导致梯度接近于0,反向传播时底层网络易出现梯度消失
批量规范化（batch normalization）的提出就是为了缓解这种状况，它将每层数据的均值修正为0，方差为1
## ResNet
ResNet是在串行的网络结构中，将之前的结果并行的加入了后面的层中，这样使得计算中增加了加法，在梯度更新时，不会因为全部的乘法操作导致梯度消失。而插入的这部分被称为残差，左图的整个块被称为残差块
![[HCIP-AI-Ascend Developer V2.0 培训教材 .pdf#page=209&rect=94,455,191,584&color=red|HCIP-AI-Ascend Developer V2.0 培训教材 , p.209|300]]
## DenseNet
从模型结构上缓解梯度消失的另一种办法是使用DenseNet（稠密连接
将之前层的特征图在通道维度上拼接成一个更大的特征
![[HCIP-AI-Ascend Developer V2.0 培训教材 .pdf#page=210&rect=251,571,407,618&color=red|HCIP-AI-Ascend Developer V2.0 培训教材 , p.210|500]]
密集连接是 DenseNet核心思想，其通过建立不同层之间的连接关系，充分利用特征信息，缓解梯度消失现象，提升网络训练效果
## 基于CNN的图像分类模型选型
![[HCIP-AI-Ascend Developer V2.0 培训教材 .pdf#page=211&rect=121,481,419,592&color=red|HCIP-AI-Ascend Developer V2.0 培训教材 , p.211]]
## 如何设计一个CNN图像分类模型
1. 在卷积神经网络发展的中，卷积是其核心技术，它降低了神经网络的参数量，并且可以提取局部特征，因此在图像分类领域，卷积神经网络是必不可少的神经网络结构。 
2. 增加神经网络能力可以从两个方面出发： 
	- 增加网络深度，例如AlexNet到VGG的发展思想 
	- 增加网络宽度，例如Inception结构 
3. 如何让网络设计的更加深（能力更强）：残差思想。它可以让网络层数设计的更深，同时还可以被较好的训练。 
4. [[全连接神经网络以及训练流程#损失函数|损失函数]]：**对于多分类问题，通常使用的是多类别交叉熵损失（Multiclass Cross-Entropy Loss），也被称为softmax loss，因为它经常与softmax激活函数一起使用作为输出层。** 
5. [[全连接神经网络以及训练流程#常见的激活函数|激活函数]]：随着网络深度增加，模型的激活函数基本以ReLU系列为主。它们可以减少梯度消失出现的概率， 使得更深的模型可以被训练。但是，这种选择也不是被固定不变的，因为ReLU函数在0点不可导，因此也可以尝试使用sigmoid等函数来代替，效果可以根据实验结果来验证。

# 目标检测
## 目标检测算法
对于日常场景中，包含多个物体的图片是更常见的，如果需要检测图中有哪些动物，需要做哪几步呢
1. 确定图片中每个物体的位置在哪里
2. 判断找到的每个物体分别是什么
目标检测分为两大系列  R-CNN系列和YOLO系列，
- R-CNN系列是基于区域检测的代表性算法
- YOLO是基于区域提取的代表性算法，另外还有著名的SSD是基于前两个系列的改进
