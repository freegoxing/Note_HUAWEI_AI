## 总览
![[Pasted image 20251030191907.png|600]]
# 有监督学习
## 线性回归
线性回归（Liner regression）:线性回归是利用数理统计中回归分析，来确定两种或两种以上变量间相互依赖的定量关系的一种统计分析方法
是一种监督学习
## 多项式回归
多项式回归，是线性回归的扩展，通常数据集的复杂度会超过用一条直线来拟合的可能性，也就是使用原始的线性回归模型会欠拟合。解决方法就是用多项式回归
## 线性回归与防止过拟合
正则项有助于减少拟合
#### 损失函数
正则项（norm）：这里这个正则项叫做L2-norm，使用这个损失函数的，线性回归也叫Ridge回归
追加了绝对值损失的线性回归叫作Loss回归
## 逻辑回归
- 是一种分类模型
- 与线性回归都是广义线性模型，在线性回归的基础上引入非线性因素（sigmoid函数），并设置了阈值
## Softmax函数
- 逻辑回归常用于二分类问题，而多分类问题，我们通常使用Softmax函数
- Softma回归数是逻辑回归的一般化，适用与k分类问题
## 决策树
决策树（decision tree）是一个树结构。每个飞叶节点表示一个特征属性上的测试，每个分支代表这个特征属性在某个值域的输出，而每一个叶子存放一个类型。
使用决策树就是从根节点开始，测试待分类项目中对应的特征属性，按照其值选择输出分支，只到到达叶子节点，将叶子节点存放的类别作为决策结果
![[Pasted image 20251030193902.png|500]]
决策树容易过拟合，需要剪枝来缩小树的结构和规模
## 支持向量机
支持向量机（Support Vector Machine, SVM）是一二分类模型，它的基本模型是定义在特征空间上的间隔最大的线性分类器
SVM的核心思路是找到一个直线，使得距离直线比较进的点，尽可能远离直线，可以使得模型具有很强的泛化能力。我们称这些点为支持向量（Support Vector）
![[Pasted image 20251030194628.png|600]]
## 非线性支持向量机
使用核函数来构建非线性支持向量机
核函数允许算法在变换后的高维度空间中拟合最大超平面
![[Pasted image 20251030194804.png|500]]
*高斯核函数用的最多*
## K最邻近算法
K最邻近算法（K-Nearest Neighbor, KNN）分类算法。是一个理论上比较成熟的方法，也是简单的机器学习算法之一
核心思想是“近朱者赤，近墨者黑”
较大的k会降低噪声对分类的影响，当时会使得类的边界不清晰
- k越大就越容易欠拟合，因为分割过于粗糙，越小就越容易过拟合，因为分割过与细腻
![[Pasted image 20251030195656.png|600]]
## 朴素贝叶斯
朴素贝叶斯（Native Bayes）:朴素贝叶斯是一种简单的多分类算法，集于贝叶斯定理
- 特征独立假设
## 集成学习
集成学习是一种机器学习范式，在这种范式中，多个学习者被训练和组合一解决同一个问题。通过使用多个学习者，集成的泛化能力可以比单个学习者强得多
#### 分类
###### Bagging
比如随机森林
对立构建数个基本学习器，然后平均他们的预测
###### Boosting
比如AdaBoost, GBDT, XGBoost
按照顺序的范式构建基本学习器，逐步减少综合学习器的偏差
#### 随机森林
随机森林（Random Forest）= Bagging + CART决策树
随机森林构建多个决策树，并将他们合并在一起
![[Pasted image 20251030200433.png|700]]
#### GBDT
GBDT是Boosting 的一种
综合模型的结构是所有基础学习器的结果相加等于预测值，其本质是下一个基础学习器去拟合误差函数对预测值的残差
# 无监督学习
## K-means
K-means算法是出入聚类个数k，以及包含n个数据对象的数据集，输出满足方差最小的标准的k个聚类的算法
## 层次聚类
层次聚类尝试在不同层次对数据进行划分，从而形成树的聚类结构
![[Pasted image 20251030200931.png]]
数根是拥有所有样本的维一聚类，叶子是仅有一个样本的聚类