# 深度神经网络
## 感知机的能力提升
多层感知机已经表现出来了它能解决复杂问题的能力（拟合复杂函数），但是它中间只能有两个单元，并且只能解决二分类的问题
## 深度神经网络
深度学习通常是指深度神经网络，深度指神经网络的层数
## 前馈神经网络
前馈神经网络（Feedforward Neural Network）是典型的深度学习模型
特点
- 第一层是输入层，是进入网络的初始数据
- 隐藏层以及输出层中的每一神经元度接受前一层全部神经元的加权和，经过激活函数输出给下一层
- 采用一种单向多层结构；整个网络中无反馈，信号称输入层向输出层单向传播，可用一个有向无环图表示
![[Pasted image 20251030204546.png]]
## 隐藏层对神经网络的影响
隐藏层越多，神经网络的分辨能力越强
# 常见的激活函数
## 深度学习网络依然需要非线性激活函数
不仅是多层感知机，深度学习网络如果不使用非线性激活函数，依然不能产生非线性能力
## 阶跃函数
使用sign函数
![[Pasted image 20251030205329.png|500]]
## Sigmoid 函数
Sigmoid函数是神经网络中经常使用的一个激活函数
$$
sigmoid(x) = \frac{1}{1+e^{-x}}
$$
把范围$(-\infty, +\infty)$映射为$(0,1)$
![[Pasted image 20251030205437.png]]
## Tanh 函数
与Sigmoid函数类似，tanh函数把范围$(-\infty, +\infty)$映射为$(-1,1)$
$$
\tanh = \frac{1-e^{-2x}}{1+e^{-2x}}
$$
![[Pasted image 20251030205710.png]]
## ReLU 函数
修正线性函数（Rectified liner unit）是最受欢迎的激活函数之一，因为它实现简单，同时在各种**预测任务**表现良好
$$
ReLU(x) =  \begin{cases}
\begin{align}
max&(0,x) &x\geq0  \\
&0&x<0
\end{align}
\end{cases}
$$
![[Pasted image 20251030210028.png]]
## Leaky ReLU 函数
Leaky ReLU函数是基于ReLU函数的改进版本，给所有负值一个非负斜率
$$
ReLU(x) =  \begin{cases}
\begin{align}
&x &x>0  \\
a&x&x\leq0
\end{align}
\end{cases}
$$
![[Pasted image 20251030210212.png|500]]
## Swish  函数
Swish函数是一种自门控激活函数
$$
swish(x)=x \sigma(\beta x)
$$
其中$\sigma(x)$是Sigmoid 函数，$\beta$是可学习的参数或一个固定的超参数
## Softmax 函数
通常是用于深度学习里面的多分类问题
它将一个含任意实数的k维向量转换为另一个在(0,1)范围里面的k维向量，并且所有元素和等于1
![[Pasted image 20251031191314.png]]
# 训练神经网络
## 从数据中学习
神经网络与多层感知机最大的区别是，在解决非线性问题时，它可以自动学习如何提取特征，从而形成解决问题的模型
## 损失函数
损失函数（Loss Function）时机器学习和深度学习中用于衡量模型预测值与真实值之间差异的函数
常用
- 均方误差：用于回归任务
- 交叉熵损失函数:用于分类任务
## 梯度下降
神经网络学习的过程中时在寻找最优参数的过程。随着参数优化，神经网络的预测值还逐渐接近真实值，进而损失值也会减小
梯度时多元函数增长最快的方向，因此需要所有参数向负梯度方向优化
#### 随机梯度下降算法（SGD）
针对原始梯度下降算法的弊端：一个常见的变体称为增量梯度下降，亦随机梯度下降。
*梯度往往会在极值附近横冲直撞*
#### 全局梯度下降算法（BGD）
*并不常用，因为收敛太慢*
#### 小批量梯度下降（MBGD）
针对上面两个梯度下降的弊端，提出一个在实际工作中最常用的梯度下降算法。它的思想时每一次使用一个小批固定尺寸（BS：Batch Size）*一般取32*
## 网络训练的流程
![[Pasted image 20251031192536.png]]
*反向传播的利用的是链式法则*
## 梯度消失和梯度爆炸问题
#### 梯度消失
当网络层数越多时候，进行反向传播求导值越小，导致梯度消失
#### 梯度爆炸
当网络层数越多时候，进行反向传播求导值越大，导致梯度爆炸
#### 解决方法
梯度裁减用于解决梯度爆炸
ReLU激活函数、LSTM神经网络用于缓解梯度消失
```ad-note
title:梯度裁减
设置一个梯度裁减的阈值，然后更新梯度时候，如果梯度超过这个阈值，就将其强制限制在这个范围内
```
# 优化器
## 加速训练
训练神经网络，如果按部就班的训练，模型学习速度较慢，因此需要使用一些特定的方法来加速训练，这些加速训练的工具就是优化器
## 优化器
在梯度下降的算法中，有各种不同的改进版本
目的
- 加快算法收敛速度
- 尽量避过和冲过局部极值
- 减少手工参数的设置难度，主要是学习率（Learning Rate）
常见的优化器
- 普通SGD优化器
- 动量优化器
- Adagrad
- RMSprop
- Adam
#### 动量优化器
###### 优点
- 增加了梯度修正的方向，减少突变
- 在梯度比较平稳的区域，可以加速收敛
- 更容易过一些狭窄的局部极值
###### 缺点
- 学习率以及动量任需要手动设置
#### Adagrad 优化器
SGD,MBGD,动量优化器的共同特点就是对于每一个参数都使用**相同的学习率**学习
Adagard的思想是应该为不同参数设置不同的学习率
###### 优点
- 学习率自动更新，随着更新次数的增加，学习率变慢
###### 缺点
- 分母会不断积累，最终学习率变得很小，算法会失去效用
#### RMSprop 优化器
RMSprop 优化器是一种改进的Adagard优化器，通过引入一个衰减系数，很好解决了Adagard优化器过早结束的问题，适合处理非平稳目标，对于RNN网络效果很好
#### Adam 优化器
Adam是从Adagrad, Adadelta上发展而来
# 正则化
## 过拟合问题
根本原因：特征维多过多，模型假设过于复杂，参数过多，训练数据过少，噪声过多，导致拟合出函数激活完美的对训练集作出预测，但是对新数据的测试集结果差
## $L_{1}$正则
通过绝对值之和来惩罚模型，倾向于产生稀疏解，即许多权重为零。
## $L_{2}$正则
通过平方和来惩罚模型，倾向于使权重接近于零，但不会强制为零，通常会导致所有特征都有一定的权重
## 提前停止训练
在训练过程中，插入对验证数据的测试
![[Pasted image 20251031195234.png|500]]
## Dropout
Dropout 是一类通用并且计算简洁的正则化。
简单来说，就是随机舍弃一部分输入