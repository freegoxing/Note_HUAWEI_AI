# 激活函数即其作用
激活函数租用作用是为神经网络引入非线性
## 引入非线性
在没有激活函数的情况下，每一层神经网络只是执行一个线性变换，即矩阵乘法和加法则
$$
y=W_{3}\left( W_{2}\left( W_{1}x+b_{1} \right)+b_{2}  \right)+b_{3} 
$$
可以通过变换成
$$
\begin{gather}
y=\left( W_{1}W_{2}W_{3} \right) x+\left( W_{3}W_{2}b_{1}+W_{3}b_{2}+b_{3} \right)  \\
y=Wx+b
\end{gather}
$$
所以无论多少层线性变换，其最终效果仍然是一个线性变换
## 解决梯度消失和梯度爆炸
#### 梯度消失
指在反向传播过程中，随着网络层数的增加，梯度在传递过程中逐渐变小，最终接近于零。这使得前面层的权重几乎不更新，导致模型无法有效地训练。梯度消失通常发生在使用Sigmoid或Tanh等激活函数时，因为这些函数的导数在输入值较大或较小时会趋近于零。
#### 梯度爆炸
指在反向传播过程中，梯度在传递过程中变得越来越大，导致权重更新时产生巨大的变化。这个现象会使模型参数变得非常不稳定，训练过程变得困难，甚至导致数值溢出。梯度爆炸通常发生在深层网络或使用较大的学习率时。
# 常见的激活函数及其图像
## 阶跃函数
使用sign函数
![400](assets/激活函数/file-20251121163637972.png)
## Sigmoid 函数
Sigmoid 曾被广泛用做激活函数，部分原因是它的数学特性与生物神经元的行为非常相似。
但 Sigmoid 在输入值非常大和非常小的情况下梯度接近0，也就是会导致梯度消失问题。这是深度神经网络训练中的一个主要挑战，目前已经逐渐被 ReLU 等现代激活函数所取代，现在其主要作用是将输入值映射到$[1, 0]$区间:
![400](assets/激活函数/file-20251121163746294.png)
## Tanh 函数
Tanh(双曲正切)激活函数是另一种常用的激活函数，其计算公式如下:
$$
\tanh = \frac{1-e^{-2x}}{1+e^{-2x}}
$$

Tanh 函数的输出范围在-1到1之间，这使其在某些应用中比 Sigmoid 函数更有用。Tanh 函数的一个优点是其输出是零中心的，这在一定程度上可以缓解梯度消失问题。然而，在输入值非常大和非常小的情况下，Tanh 函数的梯度仍然会接近0，导致梯度消失问题。
尽管如此，Tanh 函数仍然在许多神经网络中被广泛使用，尤其是在处理归一化数据时，目前也常常被用在将输入值映射到$[-1, 1]$区间的计算中。
![400](assets/激活函数/file-20251121163921185.png)
## ReLU 函数
ReLyl1](rectified linear unit，修正线性单元)激活函数是现代深度学习中最常用的激活函数之一。其计算公式如下:
$$
\mathrm{\mathrm{Re}LU} = max(0, x)
$$
ReLU 函数的输出是输入值和0之间的最大值，即当输入值为正数时，输出为输入值;当输入值为负数时，输出为0。
ReLU 函数的优点包括以下几个方面:
- 计算简单:ReLU 函数的计算非常简单，只涉及比较和选择操作。
- 缓解梯度消失问题:ReLU 函数在正值区域的梯度为1，这有效缓解了梯度消失问题，使深层神经网络的训练更加高效。
尽管 ReLu 函数在负值区域的梯度为0时，可能导致“神经元死亡”问题，但其优点使其在实际应用中仍然非常受欢迎。
此外，Rel 函数虽然在原点是不可导的，但在实际使用中可以人为地规定一个其在原点的导数值，比如0等
![400](assets/激活函数/file-20251121164142579.png)