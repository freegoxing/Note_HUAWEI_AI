# 优化方法
## 目标函数与优化概述
优化是神经网络训练过程中的一个至关重要的环节，其目标是调整网络的参数(包括权重和偏置)，以最小化一个称为目标函数(或成本函数、损失函数、误差函数)的数学表达式。这个目标函数用于度量模型预测值与真实值之间的差异。通过最小化这个目标函数，可以提升网络的预测能力。
优化问题可以表述为寻找一个参数向量$x$，使得目标函数$f(x)$达到最小值。通常，我们记为$x^{*}=\mathrm{arg}\min f(x)$，表示使$f(x)$最小化的参数向量。
优化可以理解为寻找参数最优值的过程。优化算法用于调整参数，以达到减少目标函数的值的目标。在神经网络中，最常用的优化算法之一是 梯度下降算法
## 梯度下降算法
梯度下降算法是一种基于迭代的方法，用于找到目标函数的最小值。该算法通过沿函数梯度的反方向移动，逐步接近最小值。梯度下降的原理是基于目标函数在当前点的一阶泰勒展开近似。假设我们在目标函数 f()的点 处进行一阶泰勒，展开后则有:
$$
f(x+\Delta x) \approx f(x) + \triangledown f(x)\cdot \Delta x
$$
其中，$\triangledown f(x)$是目标函数$f(x)$在点$x$处的梯度，$\Delta x$ 是一个小的偏移量。为了使$f(x)$逐步减少，我们希望选择$\Delta x$使得$f(x+\Delta x)$比$f(x)$小。由泰勒展开式可知，如果我们选择$\Delta x$与$\triangledown f(x)$的方向相反(即朝着梯度的负方向移动)，则能够减少函数值。那么，令$\Delta x=-\eta \triangledown f(x)$，其中$\eta$是一个称为学习率的正数，控制每次更新的步长。于是，我们可以得到更新规则，即:
$$
x_{new} = x-\eta\triangledown f(x)
$$
从初始点$x_{0}$开始，不断迭代应用上述更新规则，直到收敛(即梯度接近于零后者达到预定缑在的停止条件)，则有
$$
x_{k+1}=x_{k}-\eta \triangledown f(x)
$$
在梯度下降算法中，学习率是一个关键的超参数。它控制了在每一步中沿着梯度下降方向移动的步长。如果学习率过大，可能会导致算法越过最小值甚至发散:如果学习率过小，则会导致收敛速度极慢。因此，合理选择学习率对于算法的成功至关重要。
当梯度$\triangledown f(x)$接近零时，梯度下降算法达到收敛，即到达了一个临界点。这个临界点可以是局部最小值、局部最大值或鞍点。在优化过程中，我们主要关注的是局部最小值和全局最小值。由于目标函数可能具有多个局部最小值，梯度下降算法可能不会总是找到全局最小值。然而，在实际应用中，找到一个接近全局最小值的局部最小值通常已经足够，只要这个值能够提供良好的模型性能。
梯度下降算法的迭代步骤如下所示。
1. 初始化参数:从一个初始参数$x_{0}$开始。
2. 计算梯度:计算当前参数$x_{k}$处的梯度$\triangledown f(x_{k})$。
3. 更新参数:根据梯度更新参数:$x_{k+1}=x_{k}-\eta \triangledown f(x)$，其中，$\eta$是学习率，控制每次更新的步长。
4. 迭代:重复步骤(2)和步骤(3)，直到满足停止条件(例如梯度足够小或达到最大迭豐作的一次数)。
在神经网络训练过程中，常见的梯度下降类型包括批量梯度下降(batch gradientdescent，BGD)、随机梯度下降(stochastic gradient descent，SGD)和小批量梯度下降(mini-batch gradient descent，MGD)。每种类型在计算效率和收敛速度上各有优缺点
#### BGD
批量梯度下降是最基本的梯度下降方法。在每次迭代中，通过使用整个训练数据集来计算梯度，然后更新模型参数。这种方法的
- 优点:梯度计算的结果较为稳定
- 缺点:当训练数据集较大时，计算梯度的过程非常耗时。
#### SGD
随机梯度下降在每次迭代中，只使用一个样本计算梯度并更新参数。这样做可以大大加快每次迭代的速度，但由于每次更新只基于一个样本，梯度的变化会很大，可能导致收敛过程较为不稳定。*梯度往往会在极值附近横冲直撞*
#### MBGD
小批量梯度下降结合了批量梯度下降和随机梯度下降的优点。每次迭代中，通过使用一个小批量(mini-batch)的样本来计算梯度，并更新参数。这种方法在计算效率和收敛稳定性之间取得了较好的平衡，*是目前最常用的梯度下降办法。*
# 反向传播算法
反向传播算法(backpropagation)是训练人工神经网络的核心算法之一。它利用链式法则来高效地计算梯度，通过将误差从输出层传递回输入层，逐层更新每个参数的梯度，从而实现网络参数的优化。
反向传播算法主要包括两个阶段，即前向传播过程和反向传播过程。
- 前向传播过程利用前馈神经网络接受输入$x$，输入数据通过网络从输入层传递到隐藏层，逐层传递直到到达输出层，产生输出$\hat{y}$，最后利用输出$\hat{y}$和真值$y$计算误差损失。前向传播过程的目标是计算每一层的一激活值(即每一层的输出)及最终的输出。
- 反向传播过程则是利用链式法则让误差信息由网_大中在经和输出层逐层向输入层传递，利用误差信息对网络每一层的参数进行梯度求解，从而更新网络参数。
链式法则是反向传播过程的核心。假设我们有一个标量输出的目标函数 $g$，该函数依赖于输入$x$和中间变量$z$，其中，$z=f(x)$和$y=g(z)$。根据链式法则，我们可以将总与数分解为局部导数的乘积，即:
$$
\frac{\mathrm{d}y}{\mathrm{d}x} = \frac{\mathrm{d}y}{\mathrm{d}z}\cdot \frac{\mathrm{dz}}{\mathrm{d}x}
$$
# 参数正则化
在神经网络训练过程中，模型容易产生过拟合现象，即模型在训练数据上表现良好，但在未见过的测试数据上表现较差。过拟合的原因通常是由于模型的复杂度过高，导致其在训练数据中额外记录了噪声和细节。为了避免过拟合现象的发生，提高模型的泛化能力，我们可以使用参数正则化方法。正则化方法通过在损失函数中添加一个惩罚项来限制模型参数的大小，从而防止模型过于复杂。常见正则化方法有12 正则化(ridge regression)、L1 正则化(LASS0回归)、Dropout 正则化以及 Max-Norm 正则化。
## L1正则化
L1正则化是指通过在目标函数中加入权重的绝对值和来防止过拟合现象的发生
与 L2正则化不同，L1 正则化可以导致一些权重完全变为零，从而实现特征选择。这种正则化方法也被称为LASS0(least absolute shrinkage and selection operator)四归。
*通过绝对值之和来惩罚模型，倾向于产生稀疏解，即许多权重为零。*
## L2正则化
L2 正则化，也称为岭回归(ridge regression)，是指通过在目标函数中加入权重的平方和来防止过拟合现象发生
通过这一正则化项，可以在每一步梯度下降时对函数权重进行更新，使权重逐渐趋向于零，从而防止模型发生过拟合现象。
*通过平方和来惩罚模型，倾向于使权重接近于零，但不会强制为零，通常会导致所有特征都有一定的权重*
## Dropout 正则化
Dropout 正则化是指通过在训练过程中随机“丢弃”一部分神经元来防止过拟合现象的发生。在每次训练迭代中，对于每个神经元，以一定的概率$p$将其暂时移除。这相当于在训练过程中训练了不同的神经网络，从而降低了模型对特定神经元的依赖，提高了模型的泛化能力。Dropout 的实施方法如下:
- 在训练过程中，对于每一层的每个神经元，以概率将其设置为$p$;
- 在测试过程中，使用所有神经元但将每个神经元的输出按比例缩小，即乘以$1-p$，以模拟训练时的行为。
## Max-Norm 正则化
Max-Norm 正则化是指通过限制权重向量的范数来防止过拟合现象的发生。具体来说，MaxNorm 正则化会将权重向量的范数限制在一个最大值 $c$ 
这种方法可以防止权重过大，避免模型对训练数据的过拟合