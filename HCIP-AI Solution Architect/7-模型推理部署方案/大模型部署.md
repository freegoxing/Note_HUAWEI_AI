# 模型部署概述 
## AI模型部署架构示意图
![[HCIP-AI Solution Architect V1.0 培训教材.pdf#page=522&rect=143,454,413,625&color=red|HCIP-AI Solution Architect V1.0 培训教材, p.522|600]]
## 模型部署过程中的优化技术和工具
在模型部署过程中，可以采用一些优化方法和技术来提高模型的性能、可靠性和安全性
## 部署方案
#### 云端部署
云端部署是指将模型部署到云端计算资源上，用户通过网页访问或API接口调用等形式向云端服务器发出请求，云端服务器收到请求后处理并返回结果
#### 边缘/端侧部署
边缘/端侧部署则主要用于嵌入式设备，通过将模型打包封装成SDK形式，集成到嵌入式设备中，数据的处理和模型推理都在终端设备上执行。
#### 端云协同模式成为主流
端云协同：即终端和云端协同工作分流AI计算的工作负载
根据工作负载分流模式，有不同的云端混合模式： 
- 以终端为重心的混合AI，其中终端将充当锚点，用于实时推理，云端仅用于分流处理终端无法充分执行的任务以及模型的调优迭代； 
- 终端与云端协同处理的混合AI，终端用于采集数据并发送给云端，云端调用模型能力完成推理。 
## 部署形态
#### Service部署
Service部署主要用于中心服务器云端部署，一般直接以训练的引擎库作为推理服务模式。通过提供RESTful API或GRPC接口，使得用户可以方便地调用模型进行预测和推理
#### SDK部署
SDK部署则主要用于嵌入式端部署场景，以C++、Python等语言实现一套高效的前后处理和推理引擎库。开发者可以将SDK集成到嵌入式设备中，实现模型的本地推理和实时响应。
## AI处理重心从云端向终端转移
# 大模型部署概述 
## 大模型轻量化发展
从模型自身所需要的算力来看，大模型也并非只朝着越来越大的方向发展，小型化大模型对于端侧/边缘侧设备部署更友好
# 大模型部署常见优化方式
## [[模型推理#LLM大模型推理性能指标]]
## 影响模型推理速度的因素
- FLOPs（模型总的加乘运算） 
- MAC（内存访问成本） 
- 并行度（模型推理时操作的并行度越高，速度越快） 
- 计算平台（NPU，GPU，CPU等）
## 模型量化
- 模型量化是指将神经网络的浮点算法转换为定点。 
	- 低精度模型：表示模型权重数值格式为FP16（半精度浮点）或者INT8（8位的定点整数）。 
	- 常规精度模型：一般表示模型权重数值格式为FP32（32位浮点，单精度）。 
	- 混合精度（Mixed precision）：在模型中同时使用FP32和FP16的权重数值格式。 FP16减少了一半的内存大小，但有些参数或操作符必须采用FP32格式才能保持准确度。 
- 模型量化的好处： 
	- 减小模型尺寸，如8位整型量化可减少75%的模型大小； 
	- 减少存储空间，在边缘侧存储空间不足时更具有意义； 
	- 减少内存耗用，更小的模型大小意味着不需要更多的内存； 
	- 加快推理速度，访问一次FP32浮点型可以访问四次INT8整型，整型运算比浮点型运算更快； 
	- 减少设备功耗，内存耗用少了，推理速度快了，自然减少了设备功耗； 
	- 支持微处理器，有些微处理器属于8位的，低功耗运行浮点运算速度慢，需要进行8bit量化
#### 模型量化分类 
- 按照量化方案的不同： 
	- L1：data free，不使用校准集，直接将浮点参数转化成量化数，使用上非常简单，但是一般会带来很大的精度损失。 
	- L2：Calibration，基于校准集方案，通过输入少量真实数据进行统计分析。很多芯片厂商都提供这样的功能，如TensorRT、高通、海思、地平线、寒武纪。 
	- L3：Finetune，基于训练 finetune 的方案，将量化误差在训练时仿真建模，调整权重使其更适合量化。好处是能带来更大的精度提升， 缺点是要修改模型训练代码，开发周期较长。 
- 按照量化阶段的不同： 
	- Post-training quantization：PTQ（训练后量化、离线量化）。 
	- Quantization-Aware Fine-tuning：QAF（量化感知微调）。 
	- Quantization-aware training：QAT（量化感知训练，训练时量化，伪量化，在线量化）。 
- 按照量化公式的不同： 
	- 线性量化（本章节以此为例）。 
	- 非线性量化。
#### 模型量化原理 
模型量化的原理就是定点与浮点，建立了一种有效的数据映射关系。 
两个重要过程，一个是量化（Quantize），另一个是反量化（Dequantize）： 
- 量化就是将浮点型实数量化为整型数（FP32->INT8）； 
- 反量化就是将整型数转换为浮点型实数（INT8->FP32）
#### 大模型量化算法举例：QLoRA
QLoRA同时结合了模型量化Quant和LoRA参数微调两种方法，因此可以在单张48 GB的GPU上对一个 65B 的大模型做 Finetune 
## 模型剪枝
模型剪枝（Pruning）也叫模型稀疏化，不同于模型量化对每一个权重参数进行压缩，稀疏化方法是尝试直接“删除”部分权重参数。模型剪枝的原理是通过剔除模型中“不重要”的权重，使得模型减少参数量和计算量，同时尽量保证模型的精度不受影响
剪枝可以分为非结构化剪枝和结构化剪枝，二者的主要区别在于剪枝目标和由此产生的网络结构。
- 结构化剪枝剪掉基于特定规则的连接或分层结构，同时保留整体网络结构。
- 非结构化剪枝针对单个参数，会导致不规则的稀疏结构。
#### 模型剪枝的一般步骤
- 训练初始模型：首先，需要训练一个初始的大模型，通常是为了达到足够的性能水平。 
- 精准评估参数重要性：利用权重的绝对值、梯度信息等方法，深入剖析模型各参数作用，为优化决策提供有力支撑。 
- 剪枝：根据评估结果，剪掉不重要的参数或连接，可以是结构化的或非结构化的。 
- 修正和微调：进行剪枝后，需要进行一定的修正和微调， 以确保模型的性能不会显著下降
#### 非结构化剪枝 
“非结构化稀疏”（Unstructured Sparsity）也叫细粒度稀疏，主要通过对权重矩阵中的单个或整行、整列的权重值进行修剪。修剪后的新权重矩阵会变成稀疏矩阵（被修剪的值会设置为0）。 
细粒度的剪枝会带来计算特征上的“不规则”，对计算设备中的数据访问和大规模并行计算非常不友好。
#### 大模型非结构化剪枝典型算法
###### SparseGPT算法
SparseGPT（2023）方法，可以实现几个小时内在单个GPU上运行千亿以上参数的模型， 并且足够准确，可将模型修剪到50%-60%的稀疏度水平，而不会大幅度降低性能
###### Wanda算法
#### 结构化剪枝 
“结构化稀疏”（Structured Sparsity）的基本修剪单元是卷积核或权重矩阵的一个或多个Channel。由于结构化剪枝没有改变权重矩阵本身的稀疏程度，现有的计算平台和框架大都可以实现很好的支持。 
结构化稀疏（Structured Sparsity），在很多文献中也被称之为粗粒度稀疏（Coarsegrained Sparsity）或块稀疏（Block Sparsity）。
#### 大模型结构化剪枝典型算法
###### LLM-Pruner算法
思想：剪枝+少量数据+少量训练 = 高效的Large Language Models压缩
## 低秩分解 
低秩分解旨在通过将给定的权重矩阵分解成两个或多个较小维度的矩阵，从而对其进行近似
#### 大模型低秩分解典型算法
###### LoRAPrune
Low-Rank Factorization + Pruning（结合了剪枝+低秩分解的思想）
## 知识蒸馏
知识蒸馏（Knowledge Distillation，简称KD）：把复杂模型或者多个模型Ensemble（Teacher）学到的知识迁移到另一个轻量级模型（Student）上，使模型变轻量的同时（方便部署）尽量不损失性能 
![[HCIP-AI Solution Architect V1.0 培训教材.pdf#page=551&rect=128,459,427,585&color=red|HCIP-AI Solution Architect V1.0 培训教材, p.551|500]]
## 硬件选择
![[智算中心算力解决方案#通用计算 vs AI计算：“分工”不同，共建多样性计算]]
## 算子编译优化
- 计算图的优化。计算图优化的目标是对计算图进行等价的组合变换，使得减少算子的读写操作提供效率。最常见的就是算子融合
- 数据排布优化。在TensorFlow框架的输入格式NHWC，而MindSpore、 PyTorch框架下是NCHW。如何更好的布局数据能带来存储数据的访问是一个优化方向
## Continuous Batching 
Continuous Batching：一种提升LLM部署吞吐量的利器
![[HCIP-AI Solution Architect V1.0 培训教材.pdf#page=555&rect=85,476,450,618&color=red|HCIP-AI Solution Architect V1.0 培训教材, p.555]]
