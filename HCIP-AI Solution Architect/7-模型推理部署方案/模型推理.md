# 推理场景 
## 围绕体验和成本构建推理指标体系，寻求两者平衡
![[HCIP-AI Solution Architect V1.0 培训教材.pdf#page=460&rect=112,565,409,636&color=red|HCIP-AI Solution Architect V1.0 培训教材, p.460]]
## 面向业务场景的体验目标，选择不同的大模型推理部署方式
硬件选型:
- 非首token时延
- 模型名称
- 模型参数
- 输入长度
- 输出长度
- 首token时延
再加上 
- 并发用户数量
就是算力资源规模
# 大模型推理概述 
## 大模型推理趋势
长序列推理是大语言模型显著趋势
## 大模型推理现阶段的挑战
- 时延和响应时间
- 内存占用和模型大小
- 可扩展性和吞吐量
- 准确率和效率之间的权衡
- 硬件兼容和加速
## LLM推理性能影响因素
- 输入长度
- 输出长度
- 硬件算力
- 访存带宽
- 内存大小
- 互联带宽
# 大模型推理指标 
## LLM大模型推理性能指标
#### 时延
大语言模型的推理计算（computing）中，主要算子是注意力（Attention）算子和线性（Linear） 算子，两者时间占比能够达到算子执行耗时的75%以上
###### 首token时延/prefill时延TTFT（Time to Frist Token）
从接收到请求开始到第一个token生成所需的时间
###### 非首token时延/decode时延TPOT (Time Per Output Token)
在生成首词后，大模型增量推理生成一个token的平均时间时延 
###### 时延 (Latency)
数据从发出到收到系统响应所需要的总时间，通常关注3个时延指标平均时延，P95，和最大时延。 极大影响在线交互类应用的用户体验，如知识问答、对话聊天
$$
\text{时延} = TTFT\ + \ TPOT \cdot (n-1)
$$
![[HCIP-AI Solution Architect V1.0 培训教材.pdf#page=469&rect=81,474,267,622&color=red|HCIP-AI Solution Architect V1.0 培训教材, p.469|400]]
#### 吞吐量
###### 吞吐量（Throughput）
系统在单位时间内处理的数据量，影响每生成一个token的算力成本
$$
\begin{gather}
TPS\ (Tokens\ Per\ Second)\ = \frac{token\ \text{总数}}{\text{处理时间}} \\
QPS\ (Query\ Per\ Second)\ = \frac{\text{请求总数}}{\text{处理时间}} 
\end{gather}
$$
###### 最大并发数
系统可以同时处理的最大请求数量，瞬时指标
#### 有效吞吐量
###### 有效吞吐量 （Goodput） 
满足时延要求的吞吐量，是兼顾时延和吞吐量的综合指标**高吞吐量≠ 高有效吞吐量**

通过批处理大小（batchsize）来平衡时延和吞吐量：
当Batchsize较小时，batchsize的增长使吞吐量的增长快于时延的增加；但超过某个值之后，batchsize的加倍只会增加时延而不会增加吞吐量。
## LLM大模型推理精度指标
#### 绝对精度
下游数据集任务得分
#### 相对精度
logits一致性
如果两组logits（如模型迁移前和迁移后）经过softmax 后概率分布的KL（Kullback-Leibler divergence）散度接近于零，那么可以认为两组概率分布在统计意义上是等价的
当模型输出和理论输出token不一致时，可以比较第一个不一致的token对应的输出logits的概率分布的KL散度， 若KL散度趋近于0，则认为虽然输出token不一致，但两者概率分布等价，并不会影响模型采样概率。
# 大模型推理优化方式 
## 大模型推理加速的目标与优化思路
- 提高吞吐量：主要从系统的角度来看，吞吐量越高，代表LLM服务系统的资源利用率越高，成本越低。 
- 降低时延：主要从用户的视角来看，时延越低时，用户使用体验越流畅。
## 优化方法
![[HCIP-AI Solution Architect V1.0 培训教材.pdf#page=476&rect=77,454,482,641&color=red|HCIP-AI Solution Architect V1.0 培训教材, p.476]]
#### 算得快
###### KV Cache
KV Cache技术通过缓存Attention机制中的Key和Value（简称KV）来优化推理过程。
KVCache是最简单直接的优化手段，一般模型的默认实现（目前昇腾的mindformers 和modelLink）都会自带KVCache
###### KV Cache计算过程
- 预填充阶段：发生在计算第一个输出token过程中，计算时需要为每个Transformer layer计算并保存key cache和value cache；FLOPs同KV Cache关闭一致，存在大量GEMM（GEneral Matrix-Matrix multiply）操作，属于Compute-bound 类型计算。 
- 解码阶段：发生在计算第二个输出token至最后一个token过程中，这时KV Cache已存有历史键值结果，每轮推理只需读取Cache，同时将当前轮计算出的新的Key、Value追加写入至 Cache；GEMM变为GEMV（GEneral Matrix-Vector multiply）操作，FLOPs降低，推理速度相对预填充阶段变快，这时属于Memory-bound类型计算。
##### KV Cache存在问题
KV Cache是以空间换时间，当输入序列非常长的时候，需要缓存非常多K和V，显存占用非常大
###### KV Cache改进
- 共用KV cache：MQA，GQA 
	- MQA（Multi Query Attention，多查询注意力）是多头注意力的一种变体。其主要区别在于， 在MQA中不同的注意力头共享一个K和V的集合，每个头只单独保留了一份查询参数。因此K和 V的矩阵仅有一份，这大幅度减少了显存占用，使其更高效。 
	- GQA（Grouped Query Attention，分组查询注意力）是一种介于多头注意力和MQA之间的折中方案。它将查询头（Query Heads）分组，并在每组中共享一个键头（Key Head）和一个值头（Value Head）。 
- 窗口优化 
- 量化与稀疏 
- 存储与计算优化：Paged Attention
###### Kernel优化和算子融合
每次kernel调用都有一些额外开销，比如GPU和CPU之间的通信，内存拷贝等。因此可以将整个Attention的计算放进同一个kernel实现， 省略额外开销。在kernel中也可以实现一些Attention专用的优化，主流的推理库也基本都自带了高效的Kernel实现。
###### PD分离部署背景
- Prefill与Decode分离部署在不同节点上。 
- Prefill与Decode通过ROCE网络传递KV Cache。 
- Prefill与Decode分离调度与弹性扩缩。
###### PD分离部署
MindIE-Server的服务实例内，高性能请求调度和推理能力。 
MindIE-MS集群级多服务实例的管理与调度能力
##### 并行解码
###### 分布式推理
###### 流水线并行和张量并行
#### 算得少
###### [[大模型部署#模型量化|模型量化]]
###### [[大模型部署#模型剪枝|模型剪枝]]
###### [[大模型部署#低秩分解|低秩分解]]
###### [[大模型部署#知识蒸馏|蒸馏]]
# 推理系统 
## 什么是推理系统
推理系统(Inference System) 是由推理引擎、推理管理服务、模型系统等共同组成，用于部署模型，执行推理预测任务的人工智能系统。
# 推理框架 
## 业界趋势： LLM推理全栈联合优化
## Triton-简介
Triton是英伟达提供的一套集模型部署、推理、加速等解决方案，并且支持多个深度学习和机器学习框架 ， 包括 TensorRT 、 TensorFlow 、 Pytorch 、 ONNX 、 OpenVINO 、 Python、RAPIDS FIL等。
## Nvidia Triton框架
Triton 推理服务对多种查询类型提供高效的推理，支持实时查询、批处理查询、集成模型查询和音视频流查询等
## Nvidia Triton-主要特性 
- 支持多种深度学习框架； 
- 并行化模型运行； 
- 针对无状态模型的动态批处理（Dynamic batching）； 
- 针对有状态模型的顺序批处理和隐式状态管理（Sequence batching）； 
- 提供Backend API，允许添加自定义Backend或者前置/后置处理； 
- 支持HTTP/REST和GRPC通信协议； 
- 提供丰富服务指标，包括CPU/GPU利用率、服务吞吐量、服务延迟等。
## TensorRT-LLM 
TensorRT-LLM引入In-Flight batching调度方案，允许独立于其他任务进入和退出GPU。同一 GPU在处理大型计算密集型请求时，动态处理多个较小的查询，提高GPU的处理性能。
## HuggingFace TGI-简介
作为支持 HuggingFace Inference API和Hugging Chat上的LLM推理的工具，旨在支持大型语言模型的优化推理。
## vLLM介绍 
vLLM是一个开源的大模型推理加速框架，通过PagedAttention高效地管理Attention中缓存的张量，实现了比HuggingFace Transformers高14-24倍的吞吐量。
## vLLM整体架构
LLMEngine可以具体分成两个部分 
Centralized Controller： 
- 调度器的主要作用就是，在每一个推理阶段，决定要把哪些数据送给模型做推理，同时负责给这些模型分配KV Cache物理块。 
- 调度器下维护着BlockSpaceManager，它负责管理BlockAllocator（实际参与分配物理块的类）。
Distributed Workers： 
- Distributed Workers，也就是分布式系统，它的作用是将我们要使用的模型load到各块卡上，然后对 Controller传来的数据做一次推理，返回相关结果。 
- Distributed Workers指定了用什么方法管控 workers。
# 昇腾推理引擎
## [[昇腾AI基础软件#MindIE|昇腾推理引擎：全栈联合优化推理框架]]
![[昇腾AI基础软件#MindIE]]