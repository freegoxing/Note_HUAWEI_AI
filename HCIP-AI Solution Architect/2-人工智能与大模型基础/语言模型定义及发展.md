#ICL #PLMs #LLMs #Prompt #Maas #BERT #GPT #Transformer #LSTM #Word2Vec #GloVe #Instruction_Tuning
## 语言模型定义
- 给定一些词序列，预测最有可能的下一个词是什么。
- 给定词序列$𝑤_1, 𝑤_2, \dots, 𝑤_{𝑖−1}$，计算下一个词$w_{i}$的概率分布：
 $$
P(w_{i}|w_{1}, w_{2}, \dots, w_{i-1})
$$
- 是对一段文本进行概率计算的模型：
$$
P(w_{1}, w_{2}, \dots, w_{m}) = P(w_{1})P(w_{2}|w_{1})\dots P(w_{i}|w_{1}, w_{2}, \dots, w_{i-1}) \dots
$$
**语言模型**就是计算词序列（可以是短语、句子、段落）概率分布的一种模型，它的输入是文本句子，输出是该句子的概率，这个概率表明了这句话的合理程度，即这句话符合人类语言规则的程度。
## 语言模型的发展
- N-gram语言模型
```ad-note
title:N-Gram
一种基于统计语言模型的算法。它的基本思想是将文本里面的内容按照字节进行大小为N的滑动窗口操作，形成了长度是N的字节片段序列
```
- 神经网络语言模型
```ad-note
title:神经网络语言模型
基于神经网络实现的语言模型。通过使用神经网络，将单词映射为向量作为网络模型的输入来估计单词序列的联合概率。
```
- 循环神经网络语言模型
```ad-note
title:循环神经网络（RNN）语言模型 
想要解决前馈神经网络模型窗口固定的问题。其次，前馈神经网络模型假设每个输入都是独立的，但是这个假设并不合理。循环神经网络的结构能利用文字的上下文序列关系，更好地对语句之间的关系进行建模。在某种程度上也能很好地进行上下文的联系，但由于RNN可能有的长距离梯度消失问题，这个上下文的记忆能力也是有限的。
![[Pasted image 20251017090810.png]]
```ad-note
title:前馈神经网络
![[全连接神经网络以及训练流程#前馈神经网络]]
```
- Transformer语言模型 
```ad-note
title:Transformer
Transformer核心是 **Self-Attention（自注意力机制）**，这是 BERT、GPT 等现代 NLP 模型的基础。
完全抛弃 RNN，使用注意力机制并行处理序列，同时捕捉全局依赖。
Transformer 由 **Encoder-Decoder** 两部分组成：
- **Encoder**：提取输入序列的特征表示
- **Decoder**：生成输出序列（如翻译任务）
> 对于 BERT、RoBERTa，只用 **Encoder** 部分；  
> 对于 GPT，只用 **Decoder** 部分。
```
- 预训练语言模型（Pre-trained Language Models，PLMs） 
	- BERT：双向掩码语言模型 
	- GPT：纯解码器语言模型 
```ad-note
title:预训练语言模型
将预训练模型应用于下游任务时， 不需要了解太多的任务细节，不需要设计特定的神经网络结构，只需要“微调”预训练模型，即使用具体任务的标注数据在预训练语言模型上进行监督训练，就可以取得显著的性能提升

```
- 大型生成式预训练语言模型（Large Language Models，LLMs） 
	- GPT-3 
	- ChatGPT 
	- DeepSeek 
```ad-note
title:大型生成式预训练语言模型
通过语境学习 （Incontext Learning，ICL）等方法，直接使用大规模语言模型就可以在很多任务的少样本场景下取得了很好的效果。此后，研究人员们提出了面向大规模语言模型的提示词（Prompt）学习方法、模型即服务范式（Model as a Service，MaaS）、指令微调（Instruction Tuning）等方法，在不同任务上都取得了很好的效果
```
## 语言算法演进过程
#### 一阶段：标注数据驱动的深度学习模型。
######   静态词向量
如Word2Vec
```ad-note
title:静态词向量
静态词向量（static word embeddings）把每个词映射为一个固定维度的实数向量 —— 无论词出现在哪个上下文中，向量都不变。代表性方法包括 **Word2Vec（skip-gram / CBOW）**、GloVe、fastText（带子词）等。
```
```ad-note
title:Word2Vec
Word2Vec是一种基于统计方法来获得词向量的方法，它是2013年由谷歌的Mikolov提出的一套词嵌入方法。这种算法有2种训练模式
- 通过上下文来预测当前词；
- 通过当前词来预测上下文。
Word2Vec 会把每个词映射成一个 **固定长度的向量（通常 100~300 维）**，  
并让语义相似的词在向量空间中距离更近（例如 "king" 与 "queen"、"北京" 与 "上海"）。
```
```ad-note
title:Glove
GloVe的全称叫Global Vectors for Word Representation，GloVe是对Word2Vec方法的扩展，它将全局统计和Word2Vec的基于上下文的学习结合了起来。
它是一个基于全局词频统计（count-based & overall statistics）的词表征（word representation）工具，它可以把一个单词表达成一个由实数组成的向量，这些向量捕捉到了单词之间一些语义特性，比如相似性（similarity）、类比性（analogy）等。
我们通过对向量的运算， 比如欧几里得距离或者cosine相似度，可以计算出两个单词之间的语义相似性。
```
-  框架模型
	- RNN架构，如LSTM
```ad-note
title:LSTM
LSTM（长短期记忆网络, **Long Short-Term Memory**) 是理解循环神经网络（RNN）中**最核心、最经典的改进模型**。引入了“门控机制（gates）”来控制信息的记忆与遗忘。

|门/单元|作用|类比|
|---|---|---|
|**遗忘门（Forget Gate）**|决定“丢掉”哪些旧信息|像“清理记忆”|
|**输入门（Input Gate）**|决定“接收”哪些新信息|像“记录新内容”|
|**细胞状态（Cell State）**|长期记忆的存储单元|像“硬盘”|
|**输出门（Output Gate）**|决定“输出”哪些信息|像“读取记忆”|
```
#### 二阶段：自监督预训练大模型
###### 基于上下文的动态词向量
如BERT
```ad-note
title:基于上下文的动态词向量
每个词的向量表示来自**整个句子的上下文**。而并非像静态词向量一样是固定的

|句子|“bank” 的向量语义|
|---|---|
|“He deposited money in the bank.”|金融机构|
|“He sat by the river bank.”|河岸|

在静态词向量种两个"bank"的向量是固定的，而在基于上下文的动态词向量中这两个"bank"是不同的向量。它会根据**上下文**自动生成**不同的词向量**。
```ad-note
title:BERT
**BERT** 由 Google 在 **2018 年** 提出，全称是：

> **B**idirectional **E**ncoder **R**epresentations from **T**ransformers  
> （基于 Transformer 的双向编码表示）

它的核心思想是：

> 通过双向 Transformer 编码器，从**上下文两侧**同时学习词的语义表示。

💡 **一句话理解：**

> BERT 不是单向地读句子，而是“左右同时看”，生成与上下文相关的动态词向量。
```
-  模型架构
	- [[Transformer架构]]，即Self-Attention。
![[Transformer架构#Self-Attention]]
- Transformer能够解决之前RNN架构的一些核心缺陷：
	- 难以并行化，反向传播过程中需要计算整个序列；
	- 长时依赖关系建模能力不够强；
	- 模型规模难以扩大。
![[HCIP-AI Solution Architect V1.0 培训教材.pdf#page=80&rect=321,449,448,648&color=red|HCIP-AI Solution Architect V1.0 培训教材, p.80|400]]

