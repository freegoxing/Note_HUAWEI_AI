#DeepSeek  #MoE #MLA #MTP #CoT #SFT #RLHF 
## 稠密模型和稀疏模型
模型规模是提升模型性能的关键因素之一。随着基于Transformer结构的大模型不断发展， 模型的参数量越来越大（提高模型性能），如果训练和推理时全部的参数均参与计算 （即参与计算的矩阵大多为稠密矩阵）那么计算所需要的算力会越来越高。因此这类大参数量的稠密模型推理成本相对较高。 
那如何提高模型的参数，但同时又不显著增加计算需求量呢？ 
- 混合专家模型（ MoE ）是一个很好的选择！它在计算过程中，均是部分参数参与计算， 即只有少部分参数不为零。这种稀疏矩阵的计算可以减少计算资源的需求，模型单次训练和推理所需计算资源低，计算时间短。
## 混合专家模型（MoE） 
究竟什么是一个混合专家模型（MoE）呢？它的整体框架主要分为以下部分： 
- 专家网络：用于学习不同的数据。输入数据经过每个专家系统，分别会输出一个值，在该图中，每个输出标记为Oi。 
- 门控网络：决定每个专家系统输出在随机选择器里的权重。它为每个输出分配的权重为Pi。该网络同样可以被训练。  
- 随机选择器：获取多个专家系统的当前输出，结合权重获得最终输出结果。
![[HCIP-AI Solution Architect V1.0 培训教材.pdf#page=146&rect=323,474,480,633&color=red|HCIP-AI Solution Architect V1.0 培训教材, p.146|400]]
## DeepSeek-V3
DeepSeek-V3网络采用了MoE的结构，并在此基础上进行了优化。 
61层、256路由专家、1个共享专家 Kr=8，前三层MLP层总参数量671B，激活参数量37B
## DeepSeek-V3模型结构创新 
#### MLA
MLA主要作用是通过优化KV-Cache 来减少显存占用，从而提升推理性能。
####  DeepSeekMoE
- Router（路由器）
	- 路由器决定输入Uc的每一部分将路由到哪些专家 
	- 每个词元最多路由到Kr个专家节点，减少通信开销
- Experts（专家），包含256个专家，分为两类： 
	- Routed Expert：被路由器选择的专家 
	- Shared Expert：共享的专家， 用于捕捉全局特性
- Top-Kr策略： 
	- 在Nc个专家中选出响应最优的 Kr个专家 
	- 限制跨节点通信范围，优化效率 
	- 从256个专家中选9个（8+1)
#### MTP
传统预测推理速度慢 ， 逐个生成 token，预测一个token需要大量跟显存交互，加上单个token的预测倾向于局部捕捉最优，整体可能效果不好。 
核心实现思路：在解码阶段，让模型一次性通过多个顺序模块预测多个未来的token。具体在训练阶段， 一次生成多个后续的token，可以一次性学习多个位置的label，提升样本的有效利用率，提升训练效率。该技术可以用在推理阶段，一次生成多个token，实现推理速度成倍增加。
```ad-note
title:MTP(Multi-Token Prediction)
当前主流的大模型(LLMs)都是decoder-base的模型结构，也就是无论在模型训练还是在推理阶段，对于一个序列的生成过程，都是token-by-token的。每次在生成一个token的时候，都要频繁跟访存交互，加载KV-Cache，再通过多层网络做完整的前向计算。对于这样的访存密集型的任务，通常会因为访存效率形成训练或推理的瓶颈。
MTP方法，也是优化训练和推理效率的一个分支系列。
核心思想：**通过解码阶段的优化，将1-token的生成，转变成multi-token的生成，从而提升训练和推理的性能。具体来说，在训练阶段，一次生成多个后续token，可以一次学习多个位置的label，进而有效提升样本的利用效率，提升训练速度；在推理阶段通过一次生成多个token，实现成倍的推理加速来提升推理性能。**
```
## DeepSeek-R1训练流程
R1-Zero: 抛开传统的SFT、MCTS、PRM，而是在基模上应用GRPO算法进行强化学习，使模型能够自主探索CoT 来推理解决复杂问题。
```ad-note
title: SFT（监督微调)
就是让一个已经预训练好的语言模型（Pretrained Model）  
通过带“正确答案”的示例数据进行进一步训练，使它更符合人类期望。
```
```ad-note
title:MCTS（Monte Carlo Tree Search，蒙特卡洛树搜索）
 **MCTS 是一种结合“随机模拟 + 搜索树”的决策算法**，  
用来在复杂的决策空间中找到最优策略。
```ad-example
当你面对一个庞大、无法穷举的决策树（比如围棋、国际象棋、游戏）时，  
MCTS 不去把所有可能都算一遍，而是用“随机抽样 + 统计”来聪明地选出好走法。
```
```ad-note
title:PRM (Preference Ranking Model,偏好排序模型）
 它是现代大模型在训练中非常重要的一环，主要出现在 **RLHF（Reinforcement Learning from Human Feedback，人类反馈强化学习）** 之前。
是一个能判断“哪个回答更好”的模型。  
它的作用是学习 **人类的偏好**，从而为强化学习阶段提供“奖励信号”。
一句话概括：

> PRM 就是“教模型知道什么样的回答更符合人类喜欢”。
```
## DeepSeek模型蒸馏
模型蒸馏（Knowledge Distillation）是一种将大型复杂模型（教师模型，比如：DeepSeek-R1 671B）的知识迁移到小型高效模型（学生模型，比如：Qwen 7B）