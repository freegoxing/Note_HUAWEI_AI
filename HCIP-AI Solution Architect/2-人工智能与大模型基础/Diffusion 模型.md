#Diffusion #GAN #CLIP #Transformer 
## N个“一次到位
一次到位可能生成的效果比较差，质量不高。
怎么解决？N次到位。 
- 先输入随机噪声，做Denoise（去噪），中间经历非常多的Denoise。 
- Reverse Process逆向过程。 
- 正向过程：扩散，在原图上加噪声，为了生成训练数据。

## Denoise模块实际做的事情
![[HCIP-AI Solution Architect V1.0 培训教材.pdf#page=125&rect=139,484,410,608&color=red|HCIP-AI Solution Architect V1.0 培训教材, p.125|800]]
- 经过训练的噪声预测器可以对一幅添加噪声的图像进行去噪，也可以预测添加的噪声量。由于采样的噪声是可预测的，所以如果从图像中减去噪声，最后得到的图像就会更接近模型训练得到的图像。 
- 预测的是噪声，相当于预测残差，把残差减掉就是我们要的结果。 
- 输入的时候要告诉模型是第几步，e.g. 999，当前所处的阶段。类似画家绘画的过程， 画画的时候不会漫无目的，会有一个构思，先画轮廓，再慢慢填充细节。告诉模型所处的阶段，就可以更好去理解要做哪些。
## 训练Noise Predictor 
- 假设我们已经有了一张图像，生成产生一些噪声加入到图像中，然后就可以将该图像视作一个训练样例。使用相同的操作可以生成大量训练样本来训练图像生成模型中的核心组件。基于上述数据集，我们就可以训练出一个性能极佳的噪声预测器，每个训练step和其他模型的训练相似。当以某一种确定的配置运行时，噪声预测器就可以生成图像。 
- GAN和Diffusion，生成领域两个流行的模型。 
- GAN优点：保真度较高；但训练不是很稳定，需要交替训练G和D，很难训练，生成图像的多样性不好（模式崩塌）。 
```ad-note
title:GAN
GAN 是由 **Ian Goodfellow** 在 2014 年提出的，它的核心思想非常简单但强大：

> 让两个神经网络——一个“生成器 (Generator)”和一个“判别器 (Discriminator)”——相互竞争（对抗），在竞争中一起变强。

- **生成器（G）**：  
    负责“造假” → 从随机噪声中生成尽可能“以假乱真”的样本（比如生成图片）。
    
- **判别器（D）**：  
    负责“鉴定真伪” → 判断输入的样本是真的（来自真实数据集）还是假的（来自生成器）。
    

就像一个伪造货币的造假者（G）与一个鉴定员（D）之间的对抗过程：
- 造假者越来越会造假；
- 鉴定员越来越会识别；
- 直到最后，连鉴定员也分不出真假。
```
- Diffusion：早期没有流行起来，因为N次生成很慢。多样性更好。
```ad-note
title:Diffusion
这个词原本是物理里的“扩散”现象：  比如墨水滴进水里会逐渐扩散，颜色越来越模糊。
在生成模型里，我们反过来利用这个过程：

- **正向过程（Forward Process）**：  
    把一张真实图片 **逐步加噪声**，直到变成完全的随机噪声。
    
- **反向过程（Reverse Process）**：  
    训练一个模型，让它 **一步步去除噪声**，重新生成清晰图片。
    

最终我们就能从**纯噪声**生成出**逼真的图像**
```
## Stable Diffusion模型架构
![[HCIP-AI Solution Architect V1.0 培训教材.pdf#page=129&rect=81,458,424,623&color=red|HCIP-AI Solution Architect V1.0 培训教材, p.129|700]]
- Text Encoder文本编码器。 
- Generation Model扩散模型，去噪的过程。 
- latent space在压缩空间上去做生成，速度更快，生成的也是一个压缩版本的图像。 
- 通过Decoder，解码回一个原始空间的图像。 
- Stable Diffusion从功能上来说主要包括两方面：
	- 其核心功能为仅根据文本提示作为输入来生成的图像（text2img)
	- 你也可以用它对图像根据文字描述进行修改 （即输入为文本+图像）
- Stable Diffusion是一个由多个组件和模型组成的系统，而非单一的模型。当我们从模型整体的角度向模型内部观察时，可以发现，其包含一个文本理解组件用于将文本信息翻译成数字表示（numeric representation），以捕捉文本中的语义信息。 
- 然后这些信息会被提交到图像生成器（image generator）中，它的内部也包含多个组件。图像生成器主要包括两个阶段： 
	- Image information creator：该组件运行多个steps来生成图像信息，其中steps也是 Stable Diffusion接口和库中的参数，通常默认为50或100。图像信息创建器完全在图像信息空间（或潜空间）中运行，这一特性使得它比其他在像素空间工作的 Diffusion模型运行得更快。 
	- 图像解码器：图像解码器根据从图像信息创建器中获取的信息画出一幅画，整个过程只运行一次即可生成最终的像素图像。整个压缩过程，包括后续的解压、绘制图像都是通过自编码器完成的，将图像压缩到潜空间中，然后仅使用解码器使用压缩后的信息来重构。
## 文本编码器：一个Transformer语言模型
- 模型中的语言理解组件使用的是Transformer语言模型，可以将输入的文本提示转换为token嵌入向量。 
- 实验表明，相比选择更大的图像生成组件，更大的语言模型可以带来更多的图像质量提升。 
- 早期的Stable Diffusion模型使用的是OpenAI发布的经过预训练的ClipText模型，而在Stable Diffusion V2中已经转向了最新发布的、更大的CLIP模型变体OpenClip。
## Contrastive Language-Image Pre-Training（CLIP）
CLIP是图像编码器和文本编码器的组合
训练过程可以简化为拍摄图像和文字说明， 使用两个编码器对数据分别进行编码。然后使用余弦距离比较结果嵌入，刚开始训练时，即使文本描述与图像是相匹配的，它们之间的相似性肯定也是很低的。随着模型的不断更新，在后续阶段，编码器对图像和文本编码得到的嵌入会逐渐相似
## 图像解码器：Auto-encoder 
- 为了加速图像生成的过程，Stable Diffusion并没有选择在像素图像本身上运行扩散过程，而是选择在图像的压缩版本上运行，论文中也称之为「Departure to Latent Space」。 
- 整个压缩过程，包括后续的解压、绘制图像都是通过自编码器完成的，将图像压缩到潜空间中， 然后仅使用解码器使用压缩后的信息来重构
## 图像信息创建器：Diffusion U-net 
- 图像信息创建器完全在图像信息空间（或潜空间）中运行，这一特性使得它比其他在像素空间工作的Diffusion模型运行得更快。 
- 整个diffusion过程包含多个steps，其中每个step都是基于输入的latents矩阵进行操作，并生成另一个latents矩阵以更好地贴合「输入的文本」和从模型图像集中获取的「视觉信息」。
- 得到的图像并非是一张精确的原始图像，而是分布（distribution），即世界的像素排列，比如天空通常是蓝色的，人有两只眼睛，猫有尖耳朵等等，生成的具体图像风格完全取决于训练数据集。
