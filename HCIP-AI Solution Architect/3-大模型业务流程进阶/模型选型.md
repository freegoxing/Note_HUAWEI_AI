## 模型选型
 模型选型主要参考模型整体性能，以及面向的特定任务要求的模型性能。模型的性能可参考模型发布论文，或开源网站模型性能排行榜
  除此之外，huggingface上的大模型基准合集（The Big Benchmarks Collection）提供了更多的测试基准，根据面向的不同场景任务来选择
  若面向任务除了需要面向模型自身在文本理解和生成能力之外，还需要模型接入搜索引擎，RAG 或其他API等进行应用集成，则需要进一步考虑模型的综合能力，可参考huggingface上Open LLM Leaderbord排行榜，并重点关注该榜单中的常识推理指标，如HellaSwag、Winogrande，以及逻辑推理指标，如MMLU，TruthfulQA等。
  ```ad-note
title:HellaSwag 
这是一个突破常识推理界限的数据集。作者使用称为对抗过滤 (AF) 的技术来迭代生成模型的困难示例,他们创建了一个对人类来说很容易（95% 的准确率）但对机器来说具有挑战性（48%）的数据集。
  ```
```ad-note
title:Winogrande
近年来，机器学习取得了长足的进步。我们已经看到聪明的算法可以玩专业级别的电子游戏，并且模型可以生成连贯的文本。但有一件事对机器来说特别具有挑战性：常识推理。本文的作者提出了一种名为 WINOGRANDE 的数据集，该数据集包含 44k 个问题，灵感来自最初的温诺格模式挑战 (WSC)，这是一个旨在**测试机器常识能力的基准**。特别是，作者想知道这些人工智能模型是否真正获得了常识能力，或者它们是否依赖于它们训练的数据集中的虚假偏差。
```
```ad-note
title:MMLU
机器学习研究人员设计了一个新的基准来衡量文本模型的多任务准确性。该基准名为“大型多任务测试”，涵盖了 57 项广泛的任务，包括基础数学、计算机科学、法律等等。模型必须具备**广泛的世界知识和解决问题的能力**才能在此测试中获得高准确性。
```
  ```ad-note
title:TruthfulQA
随着语言模型变得越来越普遍并被部署到广泛的实际应用中，它们生成虚假陈述的能力已成为日益严重的问题。牛津大学和 OpenAI 的研究人员提出了 TruthfulQA，这是一个基准测试。此基准旨在通过测试语言模型生成 817 个精心设计的问题（涵盖 38 个类别，例如健康、法律和金融）的虚假答案的可能性来**衡量语言模型的真实性**。
  ```
## 模型获取
  - 开源模型主要从模型维护网站或仓库中获取，如github, huggingface等
  - 闭源模型一般维护在发布厂商的相关大模型服务平台，如华为昇思大模型