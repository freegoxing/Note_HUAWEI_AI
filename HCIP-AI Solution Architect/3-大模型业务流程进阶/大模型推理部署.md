# 大模型压缩量化
## 大模型压缩
模型的量化按照阶段的不同可分为
- 训练后量化
- 量化感知微调
- 训练时量化
按照公式的不同，可分为
- 线性量化
- 非线性量化。
## msModelSlim
msModelSlim，即昇腾压缩加速工具，一个以加速为目标、压缩为技术、昇腾为根本的亲和压缩工具。支持训练加速和推理加速两大场景，包括模型低秩分解、稀疏训练、训练后量化、量化感知训练等功能，昇腾AI模型开发用户可以灵活调用Python API接口，对模型进行性能调优，并支持导出不同格式模型，在昇腾AI处理器上运行。
msModelSlim针对开发者的差异化需求，提供了以下不同场景下的模型压缩方案： 
- 训练加速：模型低秩分解、模型稀疏加速训练。 
- 推理加速：
	- 训练后量化（ONNX）
	- 训练后量化（PyTorch)
	- 训练后量化（MindSpore）
	- 量化感知训练
	- Transformer类模型权重剪枝调优
	- 基于重要性评估的剪枝调优
	- 模型蒸馏。


# 大模型推理部署
## 大模型推理部署
大模型完成量化压缩后，即可对模型进行推理部署，取决于业务推理的场景，以及软硬件设备条件，主要有云端部署，边缘/端侧部署，以及端云协同部署等三种方式
## 推理部署方式介绍 
#### 云端部署
云计算平台上有大量计算资源，满足高性能计算需求，适合部署参数量较大的模型
但另一方面，受限于网页或API的带宽限制，不能满足视频流等大批量的实时推理。适合文本，图片等小批量数据推理场景。 
#### 边缘/端侧部署
这类部署主要适用于对移动性和实时性要求较高的场景，但受限于设备本身的算力和功耗，模型一般需要经过大幅度量化和压缩，或者直接选用轻量化版本的大模型，边缘/端侧部署的性能一般弱于云端部署。 
#### 端云协同部署
端云协同为终端和云端协同工作分流AI计算的工作负载，可由端侧采集数据做预处理，发送给云端推理， 或者用端侧进行实时推理，云端用于模型的迭代。典型场景为联网手机的语音助手。
## 推理部署方式对比
![[HCIP-AI Solution Architect V1.0 培训教材.pdf#page=229&rect=82,486,466,581&color=red|HCIP-AI Solution Architect V1.0 培训教材, p.229]]
