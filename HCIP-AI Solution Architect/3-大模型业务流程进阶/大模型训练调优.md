#SFT #RLHF #Full-Tuning #PEFT #LoRA #Prefix-Tuning #P-Tuning
# 大模型训练调优方法
## 大模型的训练和调优
完成数据准备和模型选型后，即可进行大模型的训练和调优。 
以大语言模型为例，目前主流训练流程主要参考OpenAI基于GPT-3基础大模型，经过有监督微调（SFT）和基于人类反馈的强化学习 （RLHF）获得用于指令生成的行业模型 InstructGPT以及用于对话的行业模型ChatGPT。
![[HCIP-AI Solution Architect V1.0 培训教材.pdf#page=203&rect=166,454,383,561&color=red|HCIP-AI Solution Architect V1.0 培训教材, p.203|700]]
## 大语言模型的训练调优过程
![[HCIP-AI Solution Architect V1.0 培训教材.pdf#page=204&rect=110,460,446,635&color=red|HCIP-AI Solution Architect V1.0 培训教材, p.204]]
## 大模型的微调
通用大模型例如ChatGPT，虽然具备不错的通用知识和能力，但到具体任务重无法达到顶尖水平，离实际业务的需求（常见的基本精度要求≥90% ）仍然还有一定差距
大模型微调的目的
- 模型层面：改善大模型在特定任务下的欠拟合问题
- 数据层面：让大模型学习特定场景/任务的语料中的信息，同时和预训练的通用语料建立联通关系，以缩小预训练数据和业务数据的差异，增强对特定业务的理解能力
- 显性结果：在特定场景/任务中的准确率提高；减少幻觉现象的发生
## 参数高效微调方法
在进行模型微调时，可选择全量微调（Full Fine-Tuning）或参数高效微调（ParameterEfficient Fine-Tuning, PEFT）。 
而由于语言模型越来越大，全量微调所需的资源成本越来越高；另一方面，单张消费级硬件显卡，往往难以承载大语言模型的全量微调。
因此一般采用参数高效微调PEFT，仅微调少量或额外的模型参数，固定大部分预训练参数，可大大降低计算和存储成本，并且最先进的PEFT技术也能实现与全量微调相当性能
![[大模型微调方案#常见的大模型微调方法]]
```ad-note
title:Full Fine-Tuning
Full-tuning是最传统、最彻底的微调方式。简单来说，就是把模型的每个零件（参数）都拿出来调一调，让它完全适应你的任务。
```
```ad-note
title:PEFT
PEFT（Parameter-Efficient Fine-Tuning）是一种在保持预训练模型大部分参数不变的情况下，通过仅调整少量额外参数来适应新任务的技术。这些额外参数可以是新添加的嵌入层、低秩矩阵或其他类型的参数，它们被用来“引导”或“调整”预训练模型的输出，以使其更适合新任务。
```
#### [[大模型微调方案#LoRA]]
基本思路：LoRA并不会直接训练原始的模型，而是将原始模型的参数固定，尝试去优化原始模型变化的秩分解矩阵。即-在模型的关键层（比如注意力机制部分）加上一组小矩阵。冻住原模型的所有参数，只调这些新加的小矩阵。任务数据跑起来时，小矩阵和原参数配合，输出结果。
LoRA优点： 
- 可泛化至全量微调，自由度高 
- 可插拔，没有额外的推理时延 
- 在GPT-3（175B）微调中，训练参数数量降低4个数量级，精度保持相当
#### [[大模型微调方案#Prefix-Tuning]]
前缀微调（prefix-tunning），用于生成任务的轻量微调。前缀微调在原始模型基础上，增加一个可被训练的 Embedding层，该Embedding层特定于模型微调面向的任务，该层称之为前缀，该层在训练过程中不断优化参数，让模型更好地理解提示词的意图
相比于传统的微调，前缀微调只优化了前缀。因此只需要存储一个大型 Transformer 和已知任务特定前缀的 Embedding层，对每个额外任务产生非常小的开销。从而提供一种自动地、能够改进模型表现的提示机制
#### [[大模型微调方案#Prompt Tuning]]
Prompt-Tuning 是为其通过设计提示词（Prompt），让预训练模型（如 GPT、BERT）更加高效地适配下游任务，逐渐替代传统的 Fine-Tuning 方法，成为参数高效微调的解决方案
#### [[大模型微调方案#P-Tuning]]
P-tuning是简单的微调方法，相当于Prefix Tuning简化版本，无需调整模型参数，而是在已有的参数中选择一部分参数作为学习参数，用于创建每个 Prompt前缀。 
P-tuning V2是P-tuning的一个优化和适应实现，但最重要的改进之一是将连续提示应用于预训练模型的每个层，而不仅仅是输入层。

## 主流大模型微调技术对比
![[HCIP-AI Solution Architect V1.0 培训教材.pdf#page=210&rect=96,482,470,618&color=red|HCIP-AI Solution Architect V1.0 培训教材, p.210]]
# 大模型开发套件
## Huggingface PEFT
Huggingface PEFT支持以下四类模型调优方法
- LoRA
- Prefix Tuning
- P-Tuninng / P-Tuningv2
- Prompt Tuning；
## DeepSpeed
另一方面，基于多卡多节点集群对大模型进行训练微调时，集群系统训练效率常常较低，集群中单个节点的效率通常达不到节点机器所能达到的最大效率的50%。
通过多种技术手段来加速训练，包括模型并行化、梯度累积、动态精度缩放、本地模式混合精度等。 DeepSpeed还提供了一些辅助工具，如分布式训练管理、内存优化和模型压缩等，以帮助开发者更好地管理和优化大规模深度学习训练任务。
![[昇腾大模型训练解决方案#DeepSpeed软件架构]]
## MindFormers
提供业内主流的Transformer类预训练模型和SOTA下游任务应用，涵盖丰富的并行特性
- 大模型开发
- 大模型训练
- 下游任务
## MindPet
MindPet（Pet：Parameter-Efficient Tuning）集成在MindFormers中，是Mindspore领域实现大模型微调的算法套件。专注于低参微调算法的开发，对外提供低参微调算法接口，使能开发者修改自己开发的模型进行低参微调
