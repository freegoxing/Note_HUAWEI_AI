## 通用计算 vs AI计算：“分工”不同，共建多样性计算
![[HCIP-AI Solution Architect V1.0 培训教材.pdf#page=262&rect=80,451,484,641&color=red|HCIP-AI Solution Architect V1.0 培训教材, p.262]]
## AI芯片
- 从技术架构来看，大致分为四个类型： 
	- CPU（Central Processing Unit，中央处理器）：是一块超大规模的集成电路，是一台计算机的运算核心（Core）和控制核心（Control Unit）。它的功能主要是解释计算机指令以及处理计算机软件中的数据。 
	- GPU（Graphics Processing Unit，图形处理器）：又称显示核心、视觉处理器、显示芯片， 是一种专门在个人电脑、工作站、游戏机和一些移动设备（如平板电脑、智能手机等）上图像运算工作的微处理器。 
	- ASIC（Application Specific Integrated Circuit，专用集成电路）：适合于某一单一用途的集成电路产品。 
	- FPGA（Field Programmable Gate Array，现场可编程门阵列）：其设计初衷是为了实现半定制芯片的功能，即硬件结构可根据需要实时配置灵活改变。
- 从业务应用来看，可以分为Training（训练）和Inference（推理）两个类型： 
	- Training环节通常需要通过大量的数据输入，或采取增强学习等非监督学习方法，训练出一个复杂的深度神经网络模型。训练过程涉及海量的训练数据和复杂的深度神经网络结构，运算量巨大，需要庞大的计算规模，对于处理器的计算能力、精度、可扩展性等性能要求很高。常用的例如NVIDIA的GPU集群、Google的TPU等。 
	- Inference环节指利用训练好的模型，使用新的数据去“推理”出各种结论。虽然Inference的计算量相比Training少很多，但仍然涉及大量的矩阵运算。在推理环节，GPU、FPGA和ASIC 都有很多应用价值
## 华为AI全栈解决方案
![[HCIP-AI Solution Architect V1.0 培训教材.pdf#page=266&rect=69,451,479,647&color=yellow|HCIP-AI Solution Architect V1.0 培训教材, p.266]]
## 昇腾（Ascend）AI处理器 
NPU（Neural-Network Processing Units，神经网络处理器）：在电路层模拟人类神经元和突触， 并且用深度学习指令集直接处理大规模的神经元和突触，一条指令完成一组神经元的处理。 
NPU的典型代表 ：华为昇腾AI芯片（Ascend）、IBM的TrueNorth。 
华为昇腾AI芯片主要有两种类型 
- 面向推理场景的芯片 
- 面向训练场景的芯片
![](assets/智算中心算力解决方案/Pasted%20image%2020251027221916.png)
#### 自研达芬奇架构+高集成SoC设计
针对 AI 计算特点 ， 包含 $16 \cdot 16 \cdot 16$ 大 Cube计算单元，矩阵乘计算效率高， 达到极致面效比
匹配AI应用场景，多维均衡考虑，应对“算” 的任务，追求性能、功耗、成本的“三维匀称”
## 芯片层：基于Da Vinci AI技术架构
## AI Core：在昇腾处理器中的位置
## CANN层：高性能算子库和算子开发工具
## 框架层：支持全场景AI计算的开源框架MindSpore
## 应用层：普惠AI+急速性能的开发平台ModelArts
![[HCIP-AI Solution Architect V1.0 培训教材.pdf#page=273&rect=86,452,463,639&color=yellow|HCIP-AI Solution Architect V1.0 培训教材, p.273]]

