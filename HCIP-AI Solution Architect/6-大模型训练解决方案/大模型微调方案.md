#Instruction_Tuning  #SFT #RLHF 
# 大模型微调算法 
## 大模型微调的种类
大模型微调的假设前提
 - 大模型仍然是欠拟合的。  
 - 模型预训练数据分布与特定任务数据分布不一致
于是有：
- Continue PreTraining（增量预训练）： 通过增加领域数据提升模型在特定领域的知识和能力。政务大模型金融大模型 L1 行业大模型 L0 基础大模型自然语言大模型 
- Supervised Finetuning（有监督微调）: 激发大模型理解领域内的各种问题并回答的能力。 
- RLHF（奖励建模、强化学习训练）：通过 RLHF可以让大模型的回答对齐人类的偏好。
## 大模型指令微调 - SFT
指令微调（Instruction Tuning）是指通过构建指令格式的实例，然后以有监督的方式对大语言模型进行微调。指令格式通常包含任务描述，一对输入输出以及示例（可选）
指令微调可以被视为有监督微调（Supervised Fine-Tuning，SFT）的一种特殊形式。但是，它们的目标依然有差别
## 大模型对齐微调 - RLHF
用人类更喜欢的数据去做训练： 
- 真实性：是虚假信息还是误导性信息？ 
- 无害性：它是否对人或环境造成身体或精神上的伤害？ 
- 有用性：它是否解决了用户的任务？
## 高效微调技术
高效微调技术可以粗略分为以下三大类：
- 增加额外参数（Addition-Based）
	- 类适配器（Adapter-like）方法
	- 软提示（Soft prompts）
- 选取一部分参数更新（Selection-Based）
- 引入重参数化（Reparametrization-Based）
## 常见的大模型微调方法
**核心思想是尽可能“不动”或者“少动”预训练模型参数**
![[HCIP-AI Solution Architect V1.0 培训教材.pdf#page=438&rect=78,489,476,569&color=red|HCIP-AI Solution Architect V1.0 培训教材, p.438]]
#### Prompt Tuning
Prompt Tuning的出发点，是基座模型（Foundation Model）的参数不变，为每个特定任务，训练一个少量参数的小模型， 在具体执行特定任务的时候按需调用
Prompt Tuning的基本原理是在输入序列X之前，增加一些特定长度的特殊Token，以增大生成期望序列的概率
![[HCIP-AI Solution Architect V1.0 培训教材.pdf#page=439&rect=166,457,389,555&color=red|HCIP-AI Solution Architect V1.0 培训教材, p.439|600]]
**Prompt Tuning是发生在Embedding这个环节的**
#### P-Tuning
P-Tuning是在Prompt Tuning的基础上，通过新增LSTM或MLP编码模块来加速模型的收敛。
P-Tuning v2通过在每一层加入Prompts tokens，实现了更多的可学习参数和更深层结构中的Prompt对模型预测的直接影响，提高了模型的灵活性和效率。
![[HCIP-AI Solution Architect V1.0 培训教材.pdf#page=440&rect=105,461,425,560&color=red|HCIP-AI Solution Architect V1.0 培训教材, p.440|600]]
#### Prefix-Tuning
Prefix Tuning的灵感来源是，基于Prompt Engineering的实践表明，在不改变大模型的前提下，在 Prompt上下文中添加适当的条件，可以引导大模型有更加出色的表现。
Prefix Tuning的出发点，跟Prompt Tuning的是类似的，只不过它们的具体实现上有一些差异
- Prompt Tuning是在Embedding环节，往输入序列X前面加特定的Token。 
- 而Prefix Tuning是在Transformer的Encoder和Decoder的网络中都加了一些特定的前缀。
- Prefix Tuning也保证了基座模型本身是没有变的，只是在推理的过程中，按需要在W前面拼接一些参数
#### LoRA
LoRA是跟Prompt Tuning和Prefix Tuning完全不相同的另一条技术路线
LoRA背后有一个假设：我们现在看到的这些大语言模型，它们都是被过度参数化的，而过度参数化的大模型背后，都有一个低维的本质模型。
LoRA基本思路：LoRA并不会直接训练原始的模型，而是将原始模型的参数固定，尝试去优化原始模型变化的秩分解矩阵。
## 主流大模型微调技术对比
- LoRA没有额外的推理延时，减少内存和存储资源消耗，大致收敛于训练原始模型，运用最广。 
- Adapter Tuning添加适配器层引入额外的计算，带来推理延迟。 
- Prefix Tuning难以优化，性能随可训练参数规模非单调变化。 
- P-Tuning前缀保留部分序列长度会减少下游任务的序列输入长度
# 大模型微调开源工具
## Huggingface 
#### Transformers
是一个基于Transformer模型结构的预训练算法库。 支持主流NLP预训练语言模型的训练和推理。 使用pipeline快速构建各种NLP任务
#### PEFT
PEFT是Huggingface开源的一个参数高效微调库，它提供了最新的参数高效微调技术，并且可以与 Transformers和Accelerate进行无缝集成
#### TRL
是Huggingface团队开发的一个用于任务导向对话系统（Task-Oriented Dialogue Systems，TODS）的强化学习框架。
TRL库专注于构建和优化交互式智能体，通过使用强化学习（Reinforcement Learning，RL） 策略来提高其在多轮对话中的性能。此项目的目标是简化任务导向型对话系统的开发， 让研究人员和开发者能够快速实验不同的强化学习算法，并将其应用到实际对话系统中。
## DeepSpeed-Chat
微软最新公布的一套工具，用于训练类ChatGPT模型。该工具基于微软的大模型训练工具 DeepSpeed，使用它可以非常简单高效地训练自己的ChatGPT
## MindPet 
MindPet是属于Mindspore领域的微调算法套件。专注于低参微调算法的开发，对外提供低参微调算法接口，使能开发者修改自己开发的模型进行低参微调。