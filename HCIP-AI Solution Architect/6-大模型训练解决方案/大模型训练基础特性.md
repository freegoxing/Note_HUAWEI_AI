## 大模型训练的特性
- 参数规模达数十亿至万亿
- 单张计算卡无法承载整个模型
- 数据规模需要PB、TB空间
- 训练周期长达数周至数月
- 开发成本高
- 训练时难以收敛，调优困难

## 大模型对训练数据的需求
大型模型通常包含数亿至万亿的参数。为了有效地调整这些参数以使其能够准确地执行特定任务 （如图像识别、自然语言处理等），需要大量的训练数据来提供足够的“信号”来指导学习过程。
```ad-tip
1 T tokens约2000GB左右，与字节有关
```
## 大模型对内（显）存资源的需求
显存需求不仅取决于模型参数规模，还与精度、推理/训练场景、优化技术等密切相关。
下面是一个7B大小的模型为例
![[HCIP-AI Solution Architect V1.0 培训教材.pdf#page=368&rect=124,477,327,569&color=red|HCIP-AI Solution Architect V1.0 培训教材, p.368|500]]
$$
\begin{gather}
\text{显存理论大小} = \text{模型大小} \times \text{每个参数占用字节}  \\
28GB = 7B \times 4\ wor ds(FP 32 \text{类型})
\end{gather}
$$
## 大模型对内（显）存资源的影响因素
![[HCIP-AI Solution Architect V1.0 培训教材.pdf#page=369&rect=89,471,472,624&color=red|HCIP-AI Solution Architect V1.0 培训教材, p.369]]
## 数据位宽介绍
#### FP32/float32
一般占用4bytes，共32位，其中1位为符号位，8位为指数位，23位为尾数位。
![[HCIP-AI Solution Architect V1.0 培训教材.pdf#page=370&rect=279,607,473,634&color=yellow|HCIP-AI Solution Architect V1.0 培训教材, p.370]]
#### FP16
是一种16位长的二进制浮点数表示方法， 采用16位进行编码存储，其中1位表示符号，5位表示指数，10位表示尾数。
![[HCIP-AI Solution Architect V1.0 培训教材.pdf#page=370&rect=286,576,418,597&color=yellow|HCIP-AI Solution Architect V1.0 培训教材, p.370|400]]
#### BF16
其指数位与FP32相同，均为8位，但小数位 （尾数位）减少到7位。这种设计使得BF16在保持一定精度的同时，大幅减少存储需求和计算成本

学习模型通常对指数的大小比对尾数敏感得多，因此BF16在实际应用中往往能提供与FP32相近的预测精度
![[HCIP-AI Solution Architect V1.0 培训教材.pdf#page=370&rect=283,530,405,552&color=yellow|HCIP-AI Solution Architect V1.0 培训教材, p.370|400]]
#### INT8、INT4
有符号整数类型，符号占1位。表示范围很小。两种类型通常用于**模型量化**。

FP32类型的参数可提供较高模型精度，但计算和存储成本高昂。降低位宽会带来模型精度的下降。

## 混合精度介绍
混合精度（Mix Precision）训练是指在训练时，对神经网络不同的运算采用不同的数值精度的运算策略
神经网络运算中，部分运算对数值精度不敏感，此时使用较低精度可以达到明显的加速效果 （ 如 conv 、 matmul 等）；而部分运算由于输入和输出的数值差异大，通常需要保留较高精度以保证结果的正确性 （ 如 log 、 softmax等）
策略
- 参数以FP32存储； 
- 正向计算过程中，遇到FP16算子，需要把算子输入和参数从FP32 cast成FP16进行计算； 
- 将Loss层设置为FP32进行计算； 
- 反向计算过程中，首先乘以Loss Scale值，避免反向梯度过小而产生下溢； 
- FP16参数参与梯度计算，其结果将被cast回FP32； 
- 除以Loss scale值，还原被放大的梯度； 
- 判断梯度是否存在溢出，如果溢出则跳过更新，否则优化器以FP32对原始参数进行更新。

## 大模型训练对计算资源的需求
![[算力规划#大模型训练对计算资源的需求]]
## 大模型训练对训练方式的需求 - 分布式并行
模型在训练过程中需要储存自身的参数和梯度，这便需要$2\psi+2\psi$的内存(我们用$\psi$代表这个参数量的数量)同时混合精度 FP32训练时，Adam需要一份FP32大小的模型拷贝，momentum和variance去储存模型的优化器状态，这需要$4\psi+4\psi+4\psi$,最终我们需要$16\psi$𝐵的内存用于训练
![[HCIP-AI Solution Architect V1.0 培训教材.pdf#page=373&rect=133,463,429,561&color=yellow|HCIP-AI Solution Architect V1.0 培训教材, p.373|600]]
```ad-note
title:Momentum
优化算法的选择和使用直接影响了模型的训练效果和性能。常见的优化算法如梯度下降法、随机梯度下降法、Adam等都采用了momentum参数。Momentum参数的主要作用是加速梯度下降，抑制震荡，从而使得训练过程更加稳定和高效。
```
```ad-note
title:方差（Variance）
通常将方差定义为使用不同的训练数据集训练出的模型对相同的输入值 x 的预测的差异，表示的是模型预测的期望值和每一个模型预测值之间的平方差异的期望值，衡量了模型预测的变化程度。
```
![[其他机器学习的重要方法#梯度下降（Gradient-Descent）]]
